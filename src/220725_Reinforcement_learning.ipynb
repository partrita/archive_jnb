{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 시작하기 앞서\n",
    "\n",
    "책 *강화학습첫걸음*을 읽고 정리한 내용입니다. 사용된 코드는 저자의 깃허브 [저장소](https://github.com/awjuliani/DeepRL-Agents)에서 가져왔습니다.\n",
    "\n",
    "\n",
    "# 1. 강화학습의 세 요소\n",
    "\n",
    "- 액션 의존성: 각 액션은 다른 보상을 가져옵니다.\n",
    "- 시간 의존성: 보상은 시간이 지연되고 나서야 주어집니다.\n",
    "- 상태 의존성: 보상은 환경의 상태에 좌우됩니다.\n",
    "\n",
    "# 밴딧 문제\n",
    "강화학습에서 가장 단순한 형태의 문제\n",
    "\n",
    "비용을 수식으로 표시하면 다음과 같습니다.\n",
    "\n",
    "$$ Loss = - log(\\pi)*A $$\n",
    "여기서 A는 **Advantage**로서 강화학습 알고리즘의 필수요소 입니다.\n",
    "\n",
    "# 마르코프 결정 과정(MDP)\n",
    "마르코프 결정과정(Markov Decision process)는 에이전트에게 완전한 문제를 제시하는 환경입니다. 어떤 과제든 MDP로 표현할 수 있습니다. 예를 들어 문을 여는 것을 상상해 봅니다. 이때의 상태는 우리의 몸의 위치와 문의 위치등이 될 수 있습니다. 행동은 우리의 몸이 만들어내는 모든 동작이 됩니다. 보상은 열린 문이 되겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 Numpy 버전 1.12.1, Tensorflow 버전은 1.3.0 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"사용한 Numpy 버전 {}, Tensorflow 버전은 {} 입니다.\".format(np.version.version , tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 기본적인 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Policy-Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0\n",
      "183.34\n",
      "185.01\n",
      "180.63\n",
      "197.15\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
    "\n",
    "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "        \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
    "            ep_history.append([s,a,r,s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            if d == True:\n",
    "                #Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "\n",
    "        \n",
    "            #Update our running tally of scores.\n",
    "        if i % 1000 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점점 시간이 늘어난 것을 확인 할 수 있습니다. \n",
    "\n",
    "# Q 러닝\n",
    "\n",
    "Q러닝은 각 상태 내에서의 값을 학습하고자 시도하며 그 상태 내에서 특정 액션을 취합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4145\n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# Implement Q-Table learning algorithm\n",
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "\n",
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[  4.21287833e-03   3.99651302e-03   1.64633532e-01   4.82147285e-03]\n",
      " [  2.37113659e-04   2.67972702e-04   5.57433680e-04   1.07443128e-01]\n",
      " [  1.07484057e-03   1.06982180e-03   3.51812188e-03   6.63885614e-02]\n",
      " [  9.14643248e-04   6.06924071e-06   6.65783676e-04   3.49788604e-02]\n",
      " [  4.64545307e-01   2.53794239e-04   2.33741614e-04   9.58178853e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.50428058e-02   9.75678622e-05   1.27225381e-04   4.50827000e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  3.20636724e-04   2.87574983e-04   3.24477041e-04   5.14538313e-01]\n",
      " [  3.00442944e-04   6.67222669e-01   1.25428587e-04   1.42980135e-03]\n",
      " [  9.06457381e-01   1.01954600e-03   7.75018879e-04   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   1.51104505e-03   6.59155290e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   9.39132791e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 네트워크 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of succesful episodes: 0.124%\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "tf.reset_default_graph()\n",
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "# Training the network\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "e = 0.1\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some statistics on network performance\n",
    "\n",
    "We can see that the network beings to consistly reach the goal around the 750 episode mark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f07b0645160>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, figsize=(9, 4))\n",
    "ax0.plot(rList)\n",
    "ax1.plot(jList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 딥 Q 네트워크(DQN)\n",
    "앞서 살펴본 Q 네트워크는 성능이 좋지 않지만, 딥 Q 네트워크는 훨씬 더 강력한 성능을 보여줍니다.\n",
    "Q 네트워크를 DQN으로 만들기 위해서는 다음과 같은 개선이 필요합니다.\n",
    "1. 단일 계층 네트워크를 다층 합성곱으로 확장\n",
    "2. 경험 리플레이의 구현\n",
    "3. 제2의 타깃 네트워크를 활용해 업데이트시 타깃 Q값을 계산\n",
    "\n",
    "> DQN은 빠르게 발전하고 있어 다양한 기법들이 나오고 있습니다.\n",
    "\n",
    "DQN에서도 널리 알려진 더블 DQN과 듀얼 DQN을 살펴보겠습니다.\n",
    "\n",
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 게임 환경 로딩\n",
    "환경을 로딩하고 그리드의 크기를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gridworld.py 파일을 불러옵니다.\n",
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 게임 시작화면의 예시입니다. 에이전트는 파란색 블럭을 이동시켜 녹색블럭은 **+1**, 빨간블럭은 **-1**의 보상을 줍니다. 블럭의 위치는 매번 랜덤하게 변합니다.\n",
    "\n",
    "### 네트워크 만들기\n",
    "\n",
    "네트워크를 클래스로 구현하는 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID',\n",
    "                                 biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID',\n",
    "                                 biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID',\n",
    "                                 biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID',\n",
    "                                 biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,\n",
    "                                                                           axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경험 리플레이\n",
    "경험과 샘플을 저장하고 네트워크를 학습시키는 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "게임 프레임의 크기와 네트워크의 매개 변수를 업데이트하는 기능을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) +\n",
    "                                                          ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 학습하기\n",
    "아래와 같이 학습 매개변수를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 1000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 2000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 1000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로로 학습을 시킵니다.\n",
    "> 시간이 오래걸리니 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "10000 5.3 0.09999999999998849\n",
      "20000 11.0 0.09999999999998849\n",
      "30000 25.3 0.09999999999998849\n",
      "40000 27.3 0.09999999999998849\n",
      "50000 28.9 0.09999999999998849\n",
      "Saved Model\n",
      "60000 30.3 0.09999999999998849\n",
      "70000 27.6 0.09999999999998849\n",
      "80000 31.4 0.09999999999998849\n",
      "90000 29.8 0.09999999999998849\n",
      "100000 29.2 0.09999999999998849\n",
      "Percent of succesful episodes: 23.241%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 200 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "학습과정을 시각화 하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0660aa4fd0>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 7. 부분관찰성과 순환 신경망\n",
    "\n",
    "\n",
    "# 8. 비동기적 어드밴티지 액터-크리틱(A3C)\n",
    "비동기적 어드밴티지 액터-크리틱(Asynchonous Advantage Actor-Critic) 알고리즘을 텐서플로에서 구현해보겠습니다. A3C는 이전 방식(DQN)보다 빠르고 강건하게 학습을 할 수 있습니다. 따라서 복잡한 상태와 액션 공간을 가지는 문제들을 해결하는 데 필수적인 알고리즘이 되었습니다. \n",
    "\n",
    "## 8.2 A3C 구현\n",
    "1. 에이전트가 전역 네트워크로 초기화합니다.\n",
    "2. 에이전트가 환경과 상호작용합니다.\n",
    "3. 에이전트가 가치와 정책비용을 계산합니다.\n",
    "4. 에이전트가 비용으로부터 경사를 구합니다.\n",
    "5. 에이전트가 경사를 이용해 전역 네트워크를 업데이트합니다.\n",
    "6. 1로 돌아갑니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1b14b4df4341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvizdoom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.signal\n",
    "%matplotlib inline\n",
    "from helper import *\n",
    "from vizdoom import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Processes Doom screen image to produce cropped and resized image. \n",
    "def process_frame(frame):\n",
    "    s = frame[10:-10,30:-30]\n",
    "    s = scipy.misc.imresize(s,[84,84])\n",
    "    s = np.reshape(s,[np.prod(s.shape)]) / 255.0\n",
    "    return s\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,s_size,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "            self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "            self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.imageIn,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "            self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "            hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.imageIn)[:1]\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,s_size,a_size,trainer,model_path,global_episodes):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(s_size,a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        \n",
    "        #The Below code is related to setting up the Doom environment\n",
    "        game.set_doom_scenario_path(\"basic.wad\") #This corresponds to the simple task we will pose our agent\n",
    "        game.set_doom_map(\"map01\")\n",
    "        game.set_screen_resolution(ScreenResolution.RES_160X120)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_render_hud(False)\n",
    "        game.set_render_crosshair(False)\n",
    "        game.set_render_weapon(True)\n",
    "        game.set_render_decals(False)\n",
    "        game.set_render_particles(False)\n",
    "        game.add_available_button(Button.MOVE_LEFT)\n",
    "        game.add_available_button(Button.MOVE_RIGHT)\n",
    "        game.add_available_button(Button.ATTACK)\n",
    "        game.add_available_game_variable(GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_X)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_Y)\n",
    "        game.set_episode_timeout(300)\n",
    "        game.set_episode_start_time(10)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_sound_enabled(False)\n",
    "        game.set_living_reward(-1)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.init()\n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        #End Doom set-up\n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs:np.vstack(observations),\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:self.batch_rnn_state[0],\n",
    "            self.local_AC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, self.batch_rnn_state,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.state_out,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,max_episode_length,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                self.env.new_episode()\n",
    "                s = self.env.get_state().screen_buffer\n",
    "                episode_frames.append(s)\n",
    "                s = process_frame(s)\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                while self.env.is_episode_finished() == False:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    r = self.env.make_action(self.actions[a]) / 100.0\n",
    "                    d = self.env.is_episode_finished()\n",
    "                    if d == False:\n",
    "                        s1 = self.env.get_state().screen_buffer\n",
    "                        episode_frames.append(s1)\n",
    "                        s1 = process_frame(s1)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        \n",
    "                    episode_buffer.append([s,a,r,s1,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d == True:\n",
    "                        break\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if self.name == 'worker_0' and episode_count % 25 == 0:\n",
    "                        time_per_step = 0.05\n",
    "                        images = np.array(episode_frames)\n",
    "                        make_gif(images,'./frames/image'+str(episode_count)+'.gif',\n",
    "                            duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                    if episode_count % 250 == 0 and self.name == 'worker_0':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_length = 300\n",
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "s_size = 7056 # Observations are greyscale frames of 84 * 84 * 1\n",
    "a_size = 3 # Agent can move Left, Right, or Fire\n",
    "load_model = False\n",
    "model_path = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists('./frames'):\n",
    "    os.makedirs('./frames')\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(s_size,a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers to number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(DoomGame(),i,s_size,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
